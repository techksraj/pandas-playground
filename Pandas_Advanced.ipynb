{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46ffa61c-81ad-40ba-ae5a-5843c4687ffe",
   "metadata": {},
   "source": [
    "# PYTHON - PANDAS TUTORIAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4dcb3967-a5fc-4321-b596-6dd85fcd1aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259e8d87-26f6-4b2e-aaf0-0ed66c1c3fde",
   "metadata": {},
   "source": [
    "# 1. Pandas - select rows\n",
    "\n",
    "Provides 2 ways to select data.\n",
    "\n",
    "* Select Rows by Integer Index - **iloc[]** -> df.iloc[start:stop:step]\n",
    "* Select Rows by Index Label - **loc[]** -> df.loc[start:stop:step]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a928437-9882-46fa-952d-9a1e7de8a7b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create DataFrame:\n",
      "     Courses    Fee Duration  Discount\n",
      "r1    Spark  20000   30days      1000\n",
      "r2  PySpark  25000   40days      2300\n",
      "r3   Hadoop  26000   35days      1500\n",
      "r4   Python  22000   40days      1200\n",
      "r5   Pandas  24000      NaN      2500\n",
      "r6   Oracle  21000     None      2100\n",
      "r7     Java  22000   55days      2000\n"
     ]
    }
   ],
   "source": [
    "technologies = {\n",
    "    'Courses':[\"Spark\",\"PySpark\",\"Hadoop\",\"Python\",\"Pandas\",\"Oracle\",\"Java\"],\n",
    "    'Fee' :[20000,25000,26000,22000,24000,21000,22000],\n",
    "    'Duration':['30days','40days','35days','40days',np.nan,None,'55days'],\n",
    "    'Discount':[1000,2300,1500,1200,2500,2100,2000]\n",
    "               }\n",
    "index_labels=['r1','r2','r3','r4','r5','r6','r7']\n",
    "df = pd.DataFrame(technologies,index=index_labels)\n",
    "print(\"Create DataFrame:\\n\", df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a492e05b-4797-4398-ace5-f934438617dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> df.iloc[2] <--\n",
      "Courses     Hadoop\n",
      "Fee          26000\n",
      "Duration    35days\n",
      "Discount      1500\n",
      "Name: r3, dtype: object\n",
      "-------------------------\n",
      "--> df.iloc[[2,3,6]] <--\n",
      "   Courses    Fee Duration  Discount\n",
      "r3  Hadoop  26000   35days      1500\n",
      "r4  Python  22000   40days      1200\n",
      "r7    Java  22000   55days      2000\n",
      "-------------------------\n",
      "--> df.iloc[1:5] <--\n",
      "    Courses    Fee Duration  Discount\n",
      "r2  PySpark  25000   40days      2300\n",
      "r3   Hadoop  26000   35days      1500\n",
      "r4   Python  22000   40days      1200\n",
      "r5   Pandas  24000      NaN      2500\n",
      "-------------------------\n",
      "--> df.iloc[:1] <--\n",
      "   Courses    Fee Duration  Discount\n",
      "r1   Spark  20000   30days      1000\n",
      "-------------------------\n",
      "--> df.iloc[:3] <--\n",
      "    Courses    Fee Duration  Discount\n",
      "r1    Spark  20000   30days      1000\n",
      "r2  PySpark  25000   40days      2300\n",
      "r3   Hadoop  26000   35days      1500\n",
      "-------------------------\n",
      "--> df.iloc[-1:] <--\n",
      "   Courses    Fee Duration  Discount\n",
      "r7    Java  22000   55days      2000\n",
      "-------------------------\n",
      "--> df.iloc[-3:] <--\n",
      "   Courses    Fee Duration  Discount\n",
      "r5  Pandas  24000      NaN      2500\n",
      "r6  Oracle  21000     None      2100\n",
      "r7    Java  22000   55days      2000\n",
      "-------------------------\n",
      "--> df.iloc[::2] <--\n",
      "   Courses    Fee Duration  Discount\n",
      "r1   Spark  20000   30days      1000\n",
      "r3  Hadoop  26000   35days      1500\n",
      "r5  Pandas  24000      NaN      2500\n",
      "r7    Java  22000   55days      2000\n",
      "-------------------------\n",
      "--> df.loc[r2] <--\n",
      "Courses     PySpark\n",
      "Fee           25000\n",
      "Duration     40days\n",
      "Discount       2300\n",
      "Name: r2, dtype: object\n",
      "-------------------------\n",
      "--> df.loc[[r2,r3,r6]] <--\n",
      "    Courses    Fee Duration  Discount\n",
      "r2  PySpark  25000   40days      2300\n",
      "r3   Hadoop  26000   35days      1500\n",
      "r6   Oracle  21000     None      2100\n",
      "-------------------------\n",
      "--> df.loc[r1:r5] <--\n",
      "    Courses    Fee Duration  Discount\n",
      "r1    Spark  20000   30days      1000\n",
      "r2  PySpark  25000   40days      2300\n",
      "r3   Hadoop  26000   35days      1500\n",
      "r4   Python  22000   40days      1200\n",
      "r5   Pandas  24000      NaN      2500\n",
      "-------------------------\n",
      "--> df.loc[r1:r5] <--\n",
      "    Courses    Fee Duration  Discount\n",
      "r1    Spark  20000   30days      1000\n",
      "r2  PySpark  25000   40days      2300\n",
      "r3   Hadoop  26000   35days      1500\n",
      "r4   Python  22000   40days      1200\n",
      "r5   Pandas  24000      NaN      2500\n",
      "-------------------------\n",
      "--> df.loc[r1:r5:2] <--\n",
      "   Courses    Fee Duration  Discount\n",
      "r1   Spark  20000   30days      1000\n",
      "r3  Hadoop  26000   35days      1500\n",
      "r5  Pandas  24000      NaN      2500\n"
     ]
    }
   ],
   "source": [
    "# Select Row by Index\n",
    "print('--> df.iloc[2] <--')\n",
    "print(df.iloc[2])\n",
    "print('-------------------------')\n",
    "# Select Rows by Index List\n",
    "print('--> df.iloc[[2,3,6]] <--')\n",
    "print(df.iloc[[2,3,6]])\n",
    "print('-------------------------')\n",
    "# Select Rows by Integer Index Range\n",
    "print('--> df.iloc[1:5] <--')\n",
    "print(df.iloc[1:5])\n",
    "print('-------------------------')\n",
    "# Select First Row\n",
    "print('--> df.iloc[:1] <--')\n",
    "print(df.iloc[:1])\n",
    "print('-------------------------')\n",
    "# Select First 3 Rows\n",
    "print('--> df.iloc[:3] <--')\n",
    "print(df.iloc[:3])\n",
    "print('-------------------------')\n",
    "# Select Last Row\n",
    "print('--> df.iloc[-1:] <--')\n",
    "print(df.iloc[-1:])\n",
    "print('-------------------------')\n",
    "# Select Last 3 Row\n",
    "print('--> df.iloc[-3:] <--')\n",
    "print(df.iloc[-3:])\n",
    "print('-------------------------')\n",
    "# Selects alternate rows\n",
    "print('--> df.iloc[::2] <--')\n",
    "print(df.iloc[::2])\n",
    "print('-------------------------')\n",
    "# Select Row by Index Label\n",
    "print('--> df.loc[r2] <--')\n",
    "print(df.loc['r2'])\n",
    "print('-------------------------')\n",
    "# Select Rows by Index Label List\n",
    "print('--> df.loc[[r2,r3,r6]] <--')\n",
    "print(df.loc[['r2','r3','r6']])\n",
    "print('-------------------------')\n",
    "# Select Rows by Label Index Range\n",
    "print('--> df.loc[r1:r5] <--')\n",
    "print(df.loc['r1':'r5'])\n",
    "print('-------------------------')\n",
    "# Select Rows by Label Index Range\n",
    "print('--> df.loc[r1:r5] <--')\n",
    "print(df.loc['r1':'r5'])\n",
    "print('-------------------------')\n",
    "# Select Alternate Rows with in Index Labels\n",
    "print('--> df.loc[r1:r5:2] <--')\n",
    "print(df.loc['r1':'r5':2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43dc08d-39e2-489e-bfe4-3d17ab2313ac",
   "metadata": {},
   "source": [
    "# 2. Pandas - select columns\n",
    "\n",
    "Same as selecting rows above, only difference being column indexes or column lables must be provided after the \",\" in both loc[] and iloc[]\n",
    "\n",
    "Provides 2 ways to select data.\n",
    "\n",
    "* Select Rows by Integer Index - **iloc[]** -> df.iloc[ : ,start:stop]\n",
    "* Select Rows by Index Label - **loc[]** -> df.loc[ : , start:stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e00bd564-b5c9-4524-ae6d-7485ea9a2cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Courses    Fee Duration\n",
      "r1    Spark  20000   30days\n",
      "r2  PySpark  25000   40days\n",
      "r3   Hadoop  26000   35days\n",
      "r4   Python  22000   40days\n",
      "r5   Pandas  24000      NaN\n",
      "r6   Oracle  21000     None\n",
      "r7     Java  22000   55days\n",
      "-------------------\n",
      "   Duration  Discount\n",
      "r1   30days      1000\n",
      "r2   40days      2300\n",
      "r3   35days      1500\n",
      "r4   40days      1200\n",
      "r5      NaN      2500\n",
      "r6     None      2100\n",
      "r7   55days      2000\n"
     ]
    }
   ],
   "source": [
    "technologies = {\n",
    "    'Courses':[\"Spark\",\"PySpark\",\"Hadoop\",\"Python\",\"Pandas\",\"Oracle\",\"Java\"],\n",
    "    'Fee' :[20000,25000,26000,22000,24000,21000,22000],\n",
    "    'Duration':['30days','40days','35days','40days',np.nan,None,'55days'],\n",
    "    'Discount':[1000,2300,1500,1200,2500,2100,2000]\n",
    "               }\n",
    "index_labels=['r1','r2','r3','r4','r5','r6','r7']\n",
    "df = pd.DataFrame(technologies,index=index_labels)\n",
    "\n",
    "print(df.loc[:, [\"Courses\",\"Fee\",\"Duration\"]]) # Selecte multiple columns\n",
    "print('-------------------')\n",
    "print(df.iloc[:,2:]) # Select From 3rd to end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a48a5d-f471-4220-b380-471c342beb0c",
   "metadata": {},
   "source": [
    "# 3. Pandas - query()\n",
    "\n",
    "### Syntax:\n",
    "DataFrame.query(expr, inplace=False, **kwargs)\n",
    "\n",
    "* **expr** – This parameter specifies the query expression string, which follows Python’s syntax for conditional expressions.\n",
    "* **inplace** – Defaults to False. When it is set to True, it updates the existing DataFrame, and query() method returns None.\n",
    "* ** **kwargs** –  This parameter allows passing additional keyword arguments to the query expression. It is optional. Keyword arguments that work with eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1b21b1d8-b2bb-4d63-8759-e5c477214053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create DataFrame:\n",
      "    Courses    Fee Duration  Discount\n",
      "0    Spark  22000   30days      1000\n",
      "1  PySpark  25000   50days      2300\n",
      "2   Hadoop  23000   30days      1000\n",
      "3   Python  24000     None      1200\n",
      "4   Pandas  26000      NaN      2500\n"
     ]
    }
   ],
   "source": [
    "technologies= {\n",
    "    'Courses':[\"Spark\",\"PySpark\",\"Hadoop\",\"Python\",\"Pandas\"],\n",
    "    'Fee' :[22000,25000,23000,24000,26000],\n",
    "    'Duration':['30days','50days','30days', None,np.nan],\n",
    "    'Discount':[1000,2300,1000,1200,2500]\n",
    "          }\n",
    "df = pd.DataFrame(technologies)\n",
    "print(\"Create DataFrame:\\n\", df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5de407de-10db-4a59-891e-77fd0f575643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After filtering the rows based on condition:\n",
      "   Courses    Fee Duration  Discount\n",
      "0   Spark  22000   30days      1000\n",
      "--------------------\n",
      "After filtering the rows based on condition:\n",
      "    Courses    Fee Duration  Discount\n",
      "0    Spark  22000   30days      1000\n",
      "1  PySpark  25000   50days      2300\n",
      "--------------------\n",
      "After filtering the rows based on multiple conditions:\n",
      "   Courses    Fee Duration  Discount\n",
      "2  Hadoop  23000   30days      1000\n",
      "3  Python  24000     None      1200\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Query all rows with Courses equals 'Spark'\n",
    "df2 = df.query(\"Courses == 'Spark'\")\n",
    "print(\"After filtering the rows based on condition:\\n\", df2)\n",
    "\n",
    "print('--------------------')\n",
    "\n",
    "# Query rows by list of values\n",
    "df2 = df.query(\"Courses in ('Spark','PySpark')\")\n",
    "print(\"After filtering the rows based on condition:\\n\", df2)\n",
    "\n",
    "print('--------------------')\n",
    "\n",
    "# Query by multiple conditions\n",
    "df2 = df.query(\"Fee >= 23000 and Fee <= 24000\")\n",
    "print(\"After filtering the rows based on multiple conditions:\\n\", df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b263d8a8-9db0-427c-a944-cd0c800b12fd",
   "metadata": {},
   "source": [
    "### Query Rows using apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "99624a4d-cea7-4f95-ba58-b1336f843ad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After filtering the rows based on condition:\n",
      "    Courses    Fee Duration  Discount\n",
      "0    Spark  22000   30days      1000\n",
      "1  PySpark  25000   50days      2300\n"
     ]
    }
   ],
   "source": [
    "# By using lambda function\n",
    "df2 = df.apply(lambda row: row[df['Courses'].isin(['Spark','PySpark'])])\n",
    "print(\"After filtering the rows based on condition:\\n\", df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44c6866-674c-4da1-beaf-b8d7b6ed7640",
   "metadata": {},
   "source": [
    "### Other Examples using df[] and loc[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "582a13d9-d31a-4134-b688-5da43823b6d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Courses    Fee Duration  Discount\n",
      "0    Spark  22000   30days      1000\n",
      "1  PySpark  25000   50days      2300\n",
      "--------------------\n",
      "   Courses    Fee Duration  Discount\n",
      "1  PySpark  25000   50days      2300\n",
      "3   Python  24000     None      1200\n",
      "4   Pandas  26000      NaN      2500\n",
      "--------------------\n",
      "   Courses    Fee Duration  Discount\n",
      "0    Spark  22000   30days      1000\n",
      "1  PySpark  25000   50days      2300\n",
      "--------------------\n",
      "   Courses    Fee Duration  Discount\n",
      "1  PySpark  25000   50days      2300\n",
      "3   Python  24000     None      1200\n",
      "4   Pandas  26000      NaN      2500\n"
     ]
    }
   ],
   "source": [
    "print(df.loc[df['Courses'].isin(['Spark','PySpark'])])\n",
    "\n",
    "print('--------------------')\n",
    "\n",
    "print(df.loc[(df['Discount'] >= 1200) & (df['Fee'] >= 23000 )])\n",
    "\n",
    "print('--------------------')\n",
    "\n",
    "# Select based on value contains\n",
    "print(df[df['Courses'].str.contains(\"Spark\")])\n",
    "\n",
    "print('--------------------')\n",
    "\n",
    "# Select startswith\n",
    "print(df[df['Courses'].str.startswith(\"P\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5022e5-123e-4a39-9991-7f55587d3b6d",
   "metadata": {},
   "source": [
    "# 4. Pandas - Get cell value\n",
    "\n",
    "* Use .loc[] to get a cell value by row label and column label.\n",
    "* Use .iloc[] to get a cell value by row and column index.\n",
    "* at[] is a faster alternative for accessing a single cell using label-based indexing.\n",
    "* .iat[] is similar to .at[], but uses integer-based indexing for faster access to a single cell.\n",
    "* Convert the DataFrame to a NumPy array and access elements by array indexing.\n",
    "* Prefer .at[] when performance is critical and only one value needs to be accessed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3ab18c70-1a06-4dc9-9663-a6e61cf82118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame\n",
    "technologies = {\n",
    "     'Courses':[\"Spark\",\"PySpark\",\"Hadoop\",\"Python\",\"pandas\"],\n",
    "     'Fee' :[24000,25000,25000,24000,24000],\n",
    "     'Duration':['30day','50days','55days', '40days','60days'],\n",
    "     'Discount':[1000,2300,1000,1200,2500]\n",
    "          }\n",
    "index_labels=['r1','r2','r3','r4','r5']\n",
    "df = pd.DataFrame(technologies, index=index_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "30c45866-6950-4230-ae18-9f355386c923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40days\n",
      "---------------------\n",
      "40days\n",
      "---------------------\n",
      "40days\n",
      "40days\n",
      "---------------------\n",
      "40days\n"
     ]
    }
   ],
   "source": [
    "# Using loc[]. Get cell value by name & index\n",
    "print(df.loc['r4']['Duration'])\n",
    "print('---------------------')\n",
    "\n",
    "# Using iloc[]. Get cell value by index & name\n",
    "print(df.iloc[3]['Duration'])\n",
    "print('---------------------')\n",
    "\n",
    "# Using DataFrame.at[]\n",
    "print(df.at['r4','Duration'])\n",
    "print(df.at[df.index[3],'Duration'])\n",
    "print('---------------------')\n",
    "\n",
    "# Using DataFrame.iat[]\n",
    "print(df.iat[3,2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9baae0-b89c-46d3-90d9-849542cbce1e",
   "metadata": {},
   "source": [
    "# 5. Pandas - Add a new Column\n",
    "\n",
    "### Syntax:\n",
    "DataFrame.assign(**kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4dd19096-d229-49f8-aedf-df124c464d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "technologies= {\n",
    "    'Courses':[\"Spark\",\"PySpark\",\"Hadoop\",\"Python\",\"Pandas\"],\n",
    "    'Fee' :[22000,25000,23000,24000,26000],\n",
    "    'Discount':[1000,2300,1000,1200,2500]\n",
    "          }\n",
    "\n",
    "df = pd.DataFrame(technologies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f884b26-30f7-49b6-85f1-9ee5302f2471",
   "metadata": {},
   "source": [
    "### Add multiple columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b5584692-d9cf-4018-9f8e-0ff24ce67790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add multiple columns to DataFrame:\n",
      "    Courses    Fee  Discount  MNCComp TutorsAssigned\n",
      "0    Spark  22000      1000     TATA        William\n",
      "1  PySpark  25000      2300      HCL          Henry\n",
      "2   Hadoop  23000      1000  Infosys        Michael\n",
      "3   Python  24000      1200   Google           John\n",
      "4   Pandas  26000      2500   Amazon          Messi\n"
     ]
    }
   ],
   "source": [
    "tutors = ['William', 'Henry', 'Michael', 'John', 'Messi']\n",
    "MNCCompanies = ['TATA','HCL','Infosys','Google','Amazon']\n",
    "df2 = df.assign(MNCComp = MNCCompanies,TutorsAssigned=tutors )\n",
    "print(\"Add multiple columns to DataFrame:\\n\", df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc48007f-0bb8-4fb8-93f4-ec7697597ee9",
   "metadata": {},
   "source": [
    "### Add a column from existing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d3289204-0115-4e6b-bbd4-21026a6c7d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add column to DataFrame:\n",
      "    Courses    Fee  Discount  Discount_Percent\n",
      "0    Spark  22000      1000          220000.0\n",
      "1  PySpark  25000      2300          575000.0\n",
      "2   Hadoop  23000      1000          230000.0\n",
      "3   Python  24000      1200          288000.0\n",
      "4   Pandas  26000      2500          650000.0\n"
     ]
    }
   ],
   "source": [
    "# Derive New Column from Existing Column\n",
    "df = pd.DataFrame(technologies)\n",
    "df2 = df.assign(Discount_Percent=lambda x: x.Fee * x.Discount / 100)\n",
    "print(\"Add column to DataFrame:\\n\", df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2dcaca-4817-4778-bc25-5b8da20114d0",
   "metadata": {},
   "source": [
    "### Append Column to Existing Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a63480fb-c355-4c00-afd2-474907cd3908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add column to DataFrame:\n",
      "    Courses    Fee  Discount MNCCompanies\n",
      "0    Spark  22000      1000         TATA\n",
      "1  PySpark  25000      2300          HCL\n",
      "2   Hadoop  23000      1000      Infosys\n",
      "3   Python  24000      1200       Google\n",
      "4   Pandas  26000      2500       Amazon\n"
     ]
    }
   ],
   "source": [
    "# Add New column to the existing DataFrame\n",
    "df = pd.DataFrame(technologies)\n",
    "MNCCompanies = ['TATA','HCL','Infosys','Google','Amazon']\n",
    "df[\"MNCCompanies\"] = MNCCompanies\n",
    "print(\"Add column to DataFrame:\\n\", df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70990182-a588-4a7d-820c-5845364662ed",
   "metadata": {},
   "source": [
    "### Add Column to Specific Position of DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9caef544-29cd-49e2-abbf-4b3db3afcef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add column to DataFrame:\n",
      "     Tutors  Courses    Fee  Discount MNCCompanies\n",
      "0  William    Spark  22000      1000         TATA\n",
      "1    Henry  PySpark  25000      2300          HCL\n",
      "2  Michael   Hadoop  23000      1000      Infosys\n",
      "3     John   Python  24000      1200       Google\n",
      "4    Messi   Pandas  26000      2500       Amazon\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Add new column at the specific position\n",
    "# Add new column to the DataFrame\n",
    "tutors = ['William', 'Henry', 'Michael', 'John', 'Messi']\n",
    "df.insert(0,'Tutors', tutors)\n",
    "print(\"Add column to DataFrame:\\n\", df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5306ab60-e7af-48b6-990b-d3a7e56a3c50",
   "metadata": {},
   "source": [
    "### Add a Column From Dictionary Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0f6b2491-4a53-4858-ad7b-ddd04a2e674d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add column to DataFrame:\n",
      "    Courses    Fee  Discount   Tutors\n",
      "0    Spark  22000      1000  William\n",
      "1  PySpark  25000      2300    Henry\n",
      "2   Hadoop  23000      1000  Michael\n",
      "3   Python  24000      1200     John\n",
      "4   Pandas  26000      2500      NaN\n"
     ]
    }
   ],
   "source": [
    "# Add new column by mapping to the existing column\n",
    "df = pd.DataFrame(technologies)\n",
    "tutors = {\"Spark\":\"William\", \"PySpark\":\"Henry\", \"Hadoop\":\"Michael\",\"Python\":\"John\", \"pandas\":\"Messi\"}\n",
    "df['Tutors'] = df['Courses'].map(tutors)\n",
    "print(\"Add column to DataFrame:\\n\", df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516c052c-8a1f-435e-90ff-fa9045f632e7",
   "metadata": {},
   "source": [
    "# 6. Pandas - Rename a column\n",
    "\n",
    "### Syntax:\n",
    "DataFrame.rename(mapper=None, index=None, columns=None, axis=None, \n",
    "       copy=True, inplace=False, level=None, errors='ignore')\n",
    "\n",
    "**mapper** – dictionary or function to rename columns and indexes.  \n",
    "**index** – dictionary or function to rename index. When using with axis param, it should be (mapper, axis=0) which is equivalent to index=mapper.  \n",
    "**columns** – dictionary or function to rename columns. When using with axis param, it should be (mapper, axis=0) which is equivalent to column=mapper.  \n",
    "**axis** – Value can be either 0 or index | 1 or columns. Default set to ‘0’.  \n",
    "**copy** – Copies the data as well. Default set to True.  \n",
    "**inplace** – Used to specify the DataFrame referred to be updated. Default to False. When used True, copy property will be ignored.  \n",
    "**level** – Used with MultiIndex. Takes Integer value. Default set to None.  \n",
    "**errors** – Take values raise or ignore. if ‘raise’ is used, raise a KeyError when a dict-like mapper, index, or column contains labels that are not present in the Index being transformed. If ‘ignore’ is used, existing keys will be renamed and extra keys will be ignored. Default set to ignore.\n",
    "\n",
    "* Pandas Rename Scenario  ->\tRename Column Example   \n",
    "* Rename columns with list  ->\tdf.columns=[‘A’,’B’,’C’]   \n",
    "* Rename column name by index  ->\tdf.columns.values[2] = “C”   \n",
    "* Rename the column using Dict  ->\tdf2=df.rename(columns={‘a’: ‘A’, ‘b’: ‘B’})   \n",
    "* Rename column using Dict & axis  ->\tdf2=df.rename({‘a’: ‘A’, ‘b’: ‘B’}, axis=1) \n",
    "* Rename column in place  ->\tdf2=df.rename({‘a’: ‘A’, ‘b’: ‘B’}, axis=’columns’)  \n",
    "* df.rename(columns={‘a’: ‘A’, ‘b’: ‘B’}, in place = True)  ->\tdf.rename(columns={‘a’: ‘A’, ‘b’: ‘B’}, inplace = True)  \n",
    "* Rename using lambda function  ->\tdf.rename(columns=lambda x: x[1:], inplace=True)  \n",
    "* Rename with error  ->\tdf.rename(columns = {‘x’:’X’}, errors = “raise”)  \n",
    "* Rename using set_axis()  ->\tdf2=df.set_axis([‘A’,’B’,’C’], axis=1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d1e556a4-83d2-4ec5-bce2-64d329ef81e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Courses_List', 'Fee', 'Duration'], dtype='object')\n",
      "Index(['Courses_List', 'Fee', 'Duration'], dtype='object')\n",
      "Index(['Courses_List', 'Courses_Fee', 'Courses_Duration'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "technologies = ({\n",
    "  'Courses':[\"Spark\",\"PySpark\",\"Hadoop\",\"Python\",\"pandas\",\"Oracle\",\"Java\"],\n",
    "  'Fee' :[20000,25000,26000,22000,24000,21000,22000],\n",
    "  'Duration':['30day', '40days' ,'35days', '40days', '60days', '50days', '55days']\n",
    "              })\n",
    "df = pd.DataFrame(technologies)\n",
    "\n",
    "# Rename a Single Column \n",
    "df2=df.rename(columns = {'Courses':'Courses_List'})\n",
    "print(df2.columns)\n",
    "\n",
    "\n",
    "# Replace existing DataFrame (inplace). This returns None.\n",
    "df.rename({'Courses':'Courses_List'}, axis='columns', inplace=True)\n",
    "print(df.columns)\n",
    "\n",
    "# Rename multiple columns\n",
    "df.rename(columns = {'Courses':'Courses_List','Fee':'Courses_Fee', \n",
    "   'Duration':'Courses_Duration'}, inplace = True)\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f6536184-586c-42f7-b366-39779bcada7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Courses_List', 'Courses_Fee', 'Courses_Duration_2'], dtype='object')\n",
      "Index(['Courses_List', 'Courses_Fee', 'Courses_Duration'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Pandas rename column by index\n",
    "df.columns.values[2] = \"Courses_Duration_2\"\n",
    "print(df.columns)\n",
    "\n",
    "\n",
    "# Rename columns with list\n",
    "column_names = ['Courses_List','Courses_Fee','Courses_Duration']\n",
    "df.columns = column_names\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0817b51e-8b05-4fe1-8a8c-0e8439cabe88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['col_Courses_List', 'col_Courses_Fee', 'col_Courses_Duration'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Rename All Column Names by adding Suffix or Prefix\n",
    "df.columns = column_names\n",
    "df.columns = ['col_'+str(col) for col in df.columns]\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "71d59319-b1cd-44db-95ea-65cbeee89954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['pre_col_Courses_List', 'pre_col_Courses_Fee',\n",
      "       'pre_col_Courses_Duration'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Add prefix to the column names\n",
    "df2=df.add_prefix('pre_')\n",
    "print(df2.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "72746a5c-f6b3-43f3-a219-d447b33e031b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['col_Courses_List_suf', 'col_Courses_Fee_suf',\n",
      "       'col_Courses_Duration_suf'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Add suffix to the column names\n",
    "df2=df.add_suffix('_suf')\n",
    "print(df2.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "039df569-d611-4340-b4e3-0e422e600cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['col_col_col_Courses_List', 'col_col_col_Courses_Fee',\n",
      "       'col_col_col_Courses_Duration'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Rename using Lambda function\n",
    "df.rename(columns=lambda x: 'col_'+x, inplace=True)\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "da3587f2-9940-4888-9e09-e968a4875545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['courses', 'fee', 'duration'], dtype='object')\n",
      "Index(['COURSES', 'FEE', 'DURATION'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Change to all lower case\n",
    "df = pd.DataFrame(technologies)\n",
    "df2=df.rename(str.lower, axis='columns')\n",
    "print(df2.columns)\n",
    "\n",
    "# Change to all upper case\n",
    "df = pd.DataFrame(technologies)\n",
    "df2=df.rename(str.upper, axis='columns')\n",
    "print(df2.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c55bca36-1be5-4fc3-9916-3035ccfd3cd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Courses', 'Fee', 'Duration'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Change column name using set_axis()\n",
    "df.set_axis(['Courses_List', 'Course_Fee', 'Course_Duration'], axis=1)\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2b1dfeec-397e-4233-9622-e94c57af6fe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Courses', 'Courses_Fee', 'Duration'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Change column name using String.replace()\n",
    "df.columns = df.columns.str.replace(\"Fee\",\"Courses_Fee\")\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "23b298c2-b019-4292-a1e5-2c38bb75cb10",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['Cour'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_38782/3414923918.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Throw Error when Rename column doesn't exists.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'Cour'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'Courses_List'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"raise\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, mapper, index, columns, axis, copy, inplace, level, errors)\u001b[0m\n\u001b[1;32m   5428\u001b[0m         \u001b[0;36m0\u001b[0m  \u001b[0;36m1\u001b[0m  \u001b[0;36m4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5429\u001b[0m         \u001b[0;36m2\u001b[0m  \u001b[0;36m2\u001b[0m  \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5430\u001b[0m         \u001b[0;36m4\u001b[0m  \u001b[0;36m3\u001b[0m  \u001b[0;36m6\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5431\u001b[0m         \"\"\"\n\u001b[0;32m-> 5432\u001b[0;31m         return super()._rename(\n\u001b[0m\u001b[1;32m   5433\u001b[0m             \u001b[0mmapper\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5434\u001b[0m             \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5435\u001b[0m             \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, mapper, index, columns, axis, copy, inplace, level, errors)\u001b[0m\n\u001b[1;32m   1028\u001b[0m                         \u001b[0mlabel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1029\u001b[0m                         \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplacements\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1030\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m                     ]\n\u001b[0;32m-> 1032\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{missing_labels} not found in axis\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1033\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m             \u001b[0mnew_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m             \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_axis_nocheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis_no\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['Cour'] not found in axis\""
     ]
    }
   ],
   "source": [
    "# Throw Error when Rename column doesn't exists.\n",
    "# df.rename(columns = {'Cour':'Courses_List'}, errors = \"raise\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271ad2e3-5328-4d44-a094-6a38a5d6b806",
   "metadata": {},
   "source": [
    "# 7. Pandas - Get Row count\n",
    "\n",
    "There are 3 ways to get the row count\n",
    "1. len(df.index)\n",
    "2. df.shape[0]\n",
    "3. df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9e7a6790-25f0-47d0-94e0-81aa82828cd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create DataFrame:\n",
      "    Courses  Courses Fee Duration  Discount\n",
      "0    Spark        22000   30days      1000\n",
      "1  PySpark        25000   50days      2300\n",
      "2   Hadoop        23000   30days      1000\n",
      "3   Python        24000     None      1200\n",
      "4   Pandas        26000      NaN      2500\n"
     ]
    }
   ],
   "source": [
    "technologies= {\n",
    "    'Courses':[\"Spark\",\"PySpark\",\"Hadoop\",\"Python\",\"Pandas\"],\n",
    "    'Courses Fee' :[22000,25000,23000,24000,26000],\n",
    "    'Duration':['30days','50days','30days', None,np.nan],\n",
    "    'Discount':[1000,2300,1000,1200,2500]\n",
    "          }\n",
    "df = pd.DataFrame(technologies)\n",
    "print(\"Create DataFrame:\\n\", df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d2109e1c-90ef-4159-8342-f5d859b098db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RangeIndex(start=0, stop=5, step=1)\n",
      "Row count is: 5\n",
      "Row count is: 5\n"
     ]
    }
   ],
   "source": [
    "# Get the row count using len(df.index)\n",
    "print(df.index)\n",
    "\n",
    "# Outputs: \n",
    "# RangeIndex(start=0, stop=5, step=1)\n",
    "\n",
    "print('Row count is:', len(df.index))\n",
    "print('Row count is:', len(df))\n",
    "\n",
    "# Outputs:\n",
    "# Row count is:5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7bbff05e-c86f-47ae-b85d-e064309c2f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RangeIndex(start=0, stop=5, step=1), Index(['Courses', 'Courses Fee', 'Duration', 'Discount'], dtype='object')]\n",
      "RangeIndex(start=0, stop=5, step=1)\n",
      "Row count is: 5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Get the row count using len(df.axes[0])\n",
    "print(df.axes)\n",
    "\n",
    "# Output:\n",
    "# [RangeIndex(start=0, stop=5, step=1), Index(['Courses', 'Courses Fee', 'Duration', 'Discount'], dtype='object')]\n",
    "\n",
    "print(df.axes[0])\n",
    "\n",
    "# Output:\n",
    "# RangeIndex(start=0, stop=5, step=1)\n",
    "\n",
    "print('Row count is:', len(df.axes[0]))\n",
    "\n",
    "# Outputs:\n",
    "# Row count is:5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1e7b3e69-4923-4138-8b6c-233beb69264a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Get row count using df.shape[0]\n",
    "df = pd.DataFrame(technologies)\n",
    "row_count = df.shape[0]  # Returns number of rows\n",
    "col_count = df.shape[1]  # Returns number of columns\n",
    "print(row_count)\n",
    "\n",
    "# Outputs:\n",
    "# Number of rows: 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "cba18091-57d1-47e2-a431-858f1846f1b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Courses        5\n",
      "Courses Fee    5\n",
      "Duration       3\n",
      "Discount       5\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Get count of each column\n",
    "print(df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b0970e-2926-4215-9b00-a3e963080fc5",
   "metadata": {},
   "source": [
    "# 8. Pandas - Iterate over rows\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "cf3439c2-911a-4b89-a4aa-2ba984e0d392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create DataFrame:    Courses    Fee Duration\n",
      "0    Spark  20000    30day\n",
      "1  PySpark  25000   40days\n",
      "2   Hadoop  26000   35days\n",
      "3   Python  22000   40days\n",
      "4   pandas  24000   60days\n",
      "5   Oracle  21000   50days\n",
      "6     Java  22000   55days\n"
     ]
    }
   ],
   "source": [
    "technologies = ({\n",
    "    'Courses':[\"Spark\",\"PySpark\",\"Hadoop\",\"Python\",\"pandas\",\"Oracle\",\"Java\"],\n",
    "    'Fee' :[20000,25000,26000,22000,24000,21000,22000],\n",
    "    'Duration':['30day', '40days' ,'35days', '40days', '60days', '50days', '55days']\n",
    "              })\n",
    "df = pd.DataFrame(technologies)\n",
    "print(\"Create DataFrame:\", df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "fd52f789-750d-4919-ac95-686844fc3bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After iterating all rows:\n",
      "\n",
      "0 20000 Spark 30day\n",
      "1 25000 PySpark 40days\n",
      "2 26000 Hadoop 35days\n",
      "3 22000 Python 40days\n",
      "4 24000 pandas 60days\n",
      "5 21000 Oracle 50days\n",
      "6 22000 Java 55days\n"
     ]
    }
   ],
   "source": [
    "# Iterate all rows \n",
    "# Using DataFrame.iterrows()\n",
    "print(\"After iterating all rows:\\n\")\n",
    "for index, row in df.iterrows():\n",
    "    print (index,row[\"Fee\"], row[\"Courses\"], row[\"Duration\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b1e0e18a-8ed4-4a51-81cc-92f1e4632bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data For First Row :\n",
      "Courses     Spark\n",
      "Fee         20000\n",
      "Duration    30day\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Row contains the column name and data\n",
    "row = next(df.iterrows())[1]\n",
    "print(\"Data For First Row :\")\n",
    "print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "13295315-6d3f-4c43-8b4b-8e42ee7cd182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 20000 Spark\n",
      "1 25000 PySpark\n",
      "2 26000 Hadoop\n",
      "3 22000 Python\n",
      "4 24000 pandas\n",
      "5 21000 Oracle\n",
      "6 22000 Java\n"
     ]
    }
   ],
   "source": [
    "# Iterate all rows \n",
    "# Using DataFrame.itertuples()\n",
    "for row in df.itertuples(index = True):\n",
    "    print (getattr(row,'Index'),getattr(row, \"Fee\"), getattr(row, \"Courses\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "941ee7ce-c842-42c4-b06b-f054a8bf046c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tution(Index=0, Courses='Spark', Fee=20000, Duration='30day')\n"
     ]
    }
   ],
   "source": [
    "# Display one row from iterator\n",
    "row = next(df.itertuples(index = True,name='Tution'))\n",
    "print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "8e266feb-4e81-4eab-aae4-c4e261479ec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      20000 Spark\n",
      "1    25000 PySpark\n",
      "2     26000 Hadoop\n",
      "3     22000 Python\n",
      "4     24000 pandas\n",
      "5     21000 Oracle\n",
      "6       22000 Java\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Another alternate approach by using DataFrame.apply()\n",
    "print(df.apply(lambda row: str(row[\"Fee\"]) + \" \" + str(row[\"Courses\"]), axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "82301be3-92af-4dd7-a062-81701afbff6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      20000 Spark\n",
      "1    25000 PySpark\n",
      "2     26000 Hadoop\n",
      "3     22000 Python\n",
      "4     24000 pandas\n",
      "5     21000 Oracle\n",
      "6       22000 Java\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Another alternate approach by using DataFrame.apply()\n",
    "print(df.apply(lambda row: str(row[\"Fee\"]) + \" \" + str(row[\"Courses\"]), axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "fdd1c0af-0fc6-4fbe-b8cb-ed4f9f1a5ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000 Spark\n",
      "25000 PySpark\n",
      "26000 Hadoop\n",
      "22000 Python\n",
      "24000 pandas\n",
      "21000 Oracle\n",
      "22000 Java\n"
     ]
    }
   ],
   "source": [
    "# Using DataFrame.index\n",
    "for idx in df.index:\n",
    "     print(df['Fee'][idx], df['Courses'][idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d16f73f0-1f20-48e0-8d96-bceaf58eb4f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000 Spark\n",
      "25000 PySpark\n",
      "26000 Hadoop\n",
      "22000 Python\n",
      "24000 pandas\n",
      "21000 Oracle\n",
      "22000 Java\n"
     ]
    }
   ],
   "source": [
    "# Another alternate approach \n",
    "# By using DataFrame.loc()\n",
    "for i in range(len(df)) :\n",
    "  print(df.loc[i, \"Fee\"], df.loc[i, \"Courses\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "02f29a9b-43b4-420c-8556-9194a82a6e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000 Spark\n",
      "25000 PySpark\n",
      "26000 Hadoop\n",
      "22000 Python\n",
      "24000 pandas\n",
      "21000 Oracle\n",
      "22000 Java\n"
     ]
    }
   ],
   "source": [
    "# Another alternate approach \n",
    "# By using DataFrame.iloc()\n",
    "for i in range(len(df)) :\n",
    "  print(df.iloc[i, 1], df.iloc[i, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "825c5a84-6d3c-4cda-bbdd-6fdf0fc2c98d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: Courses\n",
      "content: 0      Spark\n",
      "1    PySpark\n",
      "2     Hadoop\n",
      "3     Python\n",
      "4     pandas\n",
      "5     Oracle\n",
      "6       Java\n",
      "Name: Courses, dtype: object\n",
      "label: Fee\n",
      "content: 0    20000\n",
      "1    25000\n",
      "2    26000\n",
      "3    22000\n",
      "4    24000\n",
      "5    21000\n",
      "6    22000\n",
      "Name: Fee, dtype: int64\n",
      "label: Duration\n",
      "content: 0     30day\n",
      "1    40days\n",
      "2    35days\n",
      "3    40days\n",
      "4    60days\n",
      "5    50days\n",
      "6    55days\n",
      "Name: Duration, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Iterate over column by column \n",
    "# Using DataFrame.items()\n",
    "for label, content in df.items():\n",
    "    print(f'label: {label}')\n",
    "    print(f'content: {content}', sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1172f9d-ed0e-4d36-850f-dbdf14a82ba3",
   "metadata": {},
   "source": [
    "# 9. Pandas - groupby()\n",
    "\n",
    "In Pandas, you can use groupby() with the combination of sum(), count(), pivot(), transform(), aggregate(), and many more methods to perform various operations on grouped data.\n",
    "\n",
    "### Syntax\n",
    "\n",
    "DataFrame.groupby(by=None, axis=0, level=None, as_index=True, \n",
    "       sort=True, group_keys=True, squeeze=<no_default>, \n",
    "       observed=False, dropna=True)\n",
    "\n",
    "**by** – List of column names to group by  \n",
    "**axis** – Default to 0. It takes 0 or ‘index’, 1 or ‘columns’  \n",
    "**level** – Used with MultiIndex.  \n",
    "**as_index** – sql style grouped output.  \n",
    "**sort** – Default to True. Specify whether to sort after the group  \n",
    "**group_keys** – add group keys or not  \n",
    "**squeeze** – deprecated in new versions  \n",
    "**observed** – This only applies if any of the groupers are Categoricals.  \n",
    "**dropna** – Default to False. Use True to drop None/Nan on sorry keys  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "9674fb36-6e58-42e6-af0f-c5d4a643c1e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create DataFrame:\n",
      "    Courses    Fee Duration  Discount\n",
      "0    Spark  22000   30days    1000.0\n",
      "1  PySpark  25000   50days    2300.0\n",
      "2   Hadoop  23000   55days    1000.0\n",
      "3   Python  24000   40days    1200.0\n",
      "4   Pandas  26000   60days    2500.0\n",
      "5   Hadoop  25000   35days       NaN\n",
      "6    Spark  25000   30days    1400.0\n",
      "7   Python  22000   50days    1600.0\n",
      "8       NA   1500   40days       0.0\n"
     ]
    }
   ],
   "source": [
    "technologies   = ({\n",
    "    'Courses':[\"Spark\",\"PySpark\",\"Hadoop\",\"Python\",\"Pandas\",\"Hadoop\",\"Spark\",\"Python\",\"NA\"],\n",
    "    'Fee' :[22000,25000,23000,24000,26000,25000,25000,22000,1500],\n",
    "    'Duration':['30days','50days','55days','40days','60days','35days','30days','50days','40days'],\n",
    "    'Discount':[1000,2300,1000,1200,2500,None,1400,1600,0]\n",
    "          })\n",
    "df = pd.DataFrame(technologies)\n",
    "print(\"Create DataFrame:\\n\", df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "387ae499-b0eb-42c9-a93e-1a5221557ad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get sum of grouped data:\n",
      "            Fee      Duration  Discount\n",
      "Courses                               \n",
      "Hadoop   48000  55days35days    1000.0\n",
      "NA        1500        40days       0.0\n",
      "Pandas   26000        60days    2500.0\n",
      "PySpark  25000        50days    2300.0\n",
      "Python   46000  40days50days    2800.0\n",
      "Spark    47000  30days30days    2400.0\n"
     ]
    }
   ],
   "source": [
    "# Use groupby() to compute the sum\n",
    "df2 =df.groupby(['Courses']).sum()\n",
    "print(\"Get sum of grouped data:\\n\", df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "39c19fbc-8c82-4c26-8552-39b0b75a1f18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get sum of groupby multiple columns:\n",
      "                     Fee  Discount\n",
      "Courses Duration                 \n",
      "Hadoop  35days    25000       0.0\n",
      "        55days    23000    1000.0\n",
      "NA      40days     1500       0.0\n",
      "Pandas  60days    26000    2500.0\n",
      "PySpark 50days    25000    2300.0\n",
      "Python  40days    24000    1200.0\n",
      "        50days    22000    1600.0\n",
      "Spark   30days    47000    2400.0\n"
     ]
    }
   ],
   "source": [
    "# Group by multiple columns\n",
    "df2 =df.groupby(['Courses', 'Duration']).sum()\n",
    "print(\"Get sum of groupby multiple columns:\\n\", df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "fc7d6393-6430-4b7c-8f73-a9716f3af76d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After adding index to DataFrame:\n",
      "    Courses Duration    Fee  Discount\n",
      "0   Hadoop   35days  25000       0.0\n",
      "1   Hadoop   55days  23000    1000.0\n",
      "2       NA   40days   1500       0.0\n",
      "3   Pandas   60days  26000    2500.0\n",
      "4  PySpark   50days  25000    2300.0\n",
      "5   Python   40days  24000    1200.0\n",
      "6   Python   50days  22000    1600.0\n",
      "7    Spark   30days  47000    2400.0\n"
     ]
    }
   ],
   "source": [
    "# Add Row Index to the group by result\n",
    "df2 = df.groupby(['Courses','Duration']).sum().reset_index()\n",
    "print(\"After adding index to DataFrame:\\n\", df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "280e8399-8e5e-4be8-8991-d1be3bf7b731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After adding index to DataFrame:\n",
      "    Courses Duration    Fee  Discount\n",
      "0   Hadoop   35days  25000       0.0\n",
      "1   Hadoop   55days  23000    1000.0\n",
      "2       NA   40days   1500       0.0\n",
      "3   Pandas   60days  26000    2500.0\n",
      "4  PySpark   50days  25000    2300.0\n",
      "5   Python   40days  24000    1200.0\n",
      "6   Python   50days  22000    1600.0\n",
      "7    Spark   30days  47000    2400.0\n"
     ]
    }
   ],
   "source": [
    "# Add Row Index to the group by result\n",
    "df2 = df.groupby(['Courses','Duration']).sum().reset_index()\n",
    "print(\"After adding index to DataFrame:\\n\", df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "08189bd7-643b-4881-9b5f-9b30d446c697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Fee      Duration  Discount\n",
      "Courses                               \n",
      "Hadoop   48000  55days35days    1000.0\n",
      "NA        1500        40days       0.0\n",
      "Pandas   26000        60days    2500.0\n",
      "PySpark  25000        50days    2300.0\n",
      "Python   46000  40days50days    2800.0\n",
      "Spark    47000  30days30days    2400.0\n"
     ]
    }
   ],
   "source": [
    "#You can also choose whether to include NA/None/Nan in group keys or not by\n",
    "#      setting dropna parameter. By default the value of dropna set to True\n",
    "\n",
    "# Drop rows that have None/Nan on group keys\n",
    "df2=df.groupby(by=['Courses'], dropna=False).sum()\n",
    "print(df2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "7e66ef7e-0be5-4443-af38-be1e1550f9a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Fee      Duration  Discount\n",
      "Courses                               \n",
      "Spark    47000  30days30days    2400.0\n",
      "Python   46000  40days50days    2800.0\n",
      "PySpark  25000        50days    2300.0\n",
      "Pandas   26000        60days    2500.0\n",
      "NA        1500        40days       0.0\n",
      "Hadoop   48000  55days35days    1000.0\n"
     ]
    }
   ],
   "source": [
    "# Sorting group keys on descending order\n",
    "groupedDF = df.groupby('Courses',sort=False).sum()\n",
    "sortedDF=groupedDF.sort_values('Courses', ascending=False)\n",
    "print(sortedDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "904f5d8d-61d4-4a5c-b637-5eeb4bbc3abe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Fee      Duration  Discount\n",
      "Courses                               \n",
      "Hadoop   48000  55days35days    1000.0\n",
      "NA        1500        40days       0.0\n",
      "Pandas   26000        60days    2500.0\n",
      "PySpark  25000        50days    2300.0\n",
      "Python   46000  40days50days    2800.0\n",
      "Spark    47000  30days30days    2400.0\n"
     ]
    }
   ],
   "source": [
    "# Sorting group keys on descending order\n",
    "groupedDF = df.groupby('Courses',sort=False).sum()\n",
    "sortedDF=groupedDF.sort_values('Courses', ascending=True)\n",
    "print(sortedDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "293a6f79-d172-4d56-9e79-27892fd8b8f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Courses    Fee Duration  Discount\n",
      "Courses                                     \n",
      "Hadoop  2   Hadoop  23000   55days    1000.0\n",
      "        5   Hadoop  25000   35days       NaN\n",
      "NA      8       NA   1500   40days       0.0\n",
      "Pandas  4   Pandas  26000   60days    2500.0\n",
      "PySpark 1  PySpark  25000   50days    2300.0\n",
      "Python  7   Python  22000   50days    1600.0\n",
      "        3   Python  24000   40days    1200.0\n",
      "Spark   0    Spark  22000   30days    1000.0\n",
      "        6    Spark  25000   30days    1400.0\n"
     ]
    }
   ],
   "source": [
    "# Using apply() & lambda\n",
    "df2 = df.groupby('Courses').apply(lambda x: x.sort_values('Fee'))\n",
    "print(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "9229e5b3-02dc-4b3d-b0d3-b428e1fddf48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After applying multiple aggregations on grouped data:\n",
      "            min    max  count   median\n",
      "Courses                              \n",
      "Hadoop   23000  25000      2  24000.0\n",
      "NA        1500   1500      1   1500.0\n",
      "Pandas   26000  26000      1  26000.0\n",
      "PySpark  25000  25000      1  25000.0\n",
      "Python   22000  24000      2  23000.0\n",
      "Spark    22000  25000      2  23500.0\n"
     ]
    }
   ],
   "source": [
    "# Groupby & multiple aggregations\n",
    "result = df.groupby('Courses')['Fee'].aggregate(['min','max','count','median'])\n",
    "print(\"After applying multiple aggregations on grouped data:\\n\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "c0c42149-18f1-459c-86d3-efcf52b25007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After applying multiple aggregations on grouped data:\n",
      "         Duration    Fee       \n",
      "           count    min    max\n",
      "Courses                       \n",
      "Hadoop         2  23000  25000\n",
      "NA             1   1500   1500\n",
      "Pandas         1  26000  26000\n",
      "PySpark        1  25000  25000\n",
      "Python         2  22000  24000\n",
      "Spark          2  22000  25000\n"
     ]
    }
   ],
   "source": [
    "# Groupby multiple columns & multiple aggregations\n",
    "result = df.groupby('Courses').aggregate({'Duration':'count','Fee':['min','max']})\n",
    "print(\"After applying multiple aggregations on grouped data:\\n\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7111f442-b1ad-4568-9f18-2c83da5f6f76",
   "metadata": {},
   "source": [
    "# 10. Pandas - Shuffle DataFrame rows\n",
    "\n",
    "By using pandas.DataFrame.sample() method you can shuffle the DataFrame rows randomly, if you are using the NumPy module you can use the permutation() method to change the order of the rows also called the shuffle. Python also has other packages like sklearn that has a method shuffle() to shuffle the order of rows in DataFrame.\n",
    "\n",
    "* Shuffling DataFrame rows helps in randomizing the order of data, which can be crucial for certain statistical analyses and machine learning tasks.\n",
    "* The DataFrame.sample() method in Pandas provides a convenient way to shuffle DataFrame rows efficiently without modifying the original DataFrame.\n",
    "* Shuffling DataFrame rows can help in enhancing the diversity of data subsets, thereby improving the generalization ability of machine learning models.\n",
    "* The DataFrame.sample() method facilitates row shuffling with parameters such as frac to specify the fraction of rows or n to define the exact number of rows to sample.\n",
    "* To shuffle the DataFrame in place, use the DataFrame.sample() method with the frac=1 parameter.\n",
    "* For large datasets, shuffling can be memory-intensive, necessitating careful consideration of computational resources, especially in distributed computing environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "76eed6a7-b56c-47b5-a0ba-cc9b227bedcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Courses    Fee Duration  Discount\n",
      "0    Spark  20000   30days      1000\n",
      "1  PySpark  25000   40days      2300\n",
      "2   Hadoop  26000   35days      1500\n",
      "3   Python  22000   40days      1200\n",
      "4   pandas  24000   60days      2500\n",
      "5   Oracle  21000   50days      2100\n",
      "6     Java  22000   55days      2000\n"
     ]
    }
   ],
   "source": [
    "technologies = {\n",
    "    'Courses':[\"Spark\",\"PySpark\",\"Hadoop\",\"Python\",\"pandas\",\"Oracle\",\"Java\"],\n",
    "    'Fee' :[20000,25000,26000,22000,24000,21000,22000],\n",
    "    'Duration':['30days','40days','35days','40days','60days','50days','55days'],\n",
    "    'Discount':[1000,2300,1500,1200,2500,2100,2000]\n",
    "               }\n",
    "df = pd.DataFrame(technologies)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "46dd7e34-d87c-4b49-8e72-035513c9051b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Courses    Fee Duration  Discount\n",
      "2   Hadoop  26000   35days      1500\n",
      "4   pandas  24000   60days      2500\n",
      "3   Python  22000   40days      1200\n",
      "6     Java  22000   55days      2000\n",
      "1  PySpark  25000   40days      2300\n",
      "5   Oracle  21000   50days      2100\n",
      "0    Spark  20000   30days      1000\n"
     ]
    }
   ],
   "source": [
    "# Shuffle the DataFrame rows & return all rows\n",
    "df1 = df.sample(frac = 1) # frac -> decimal -> None: returns 1 record -> 0.5 -> 50 % rows\n",
    "print(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "d4c8f689-8a1d-48a0-8ae1-1a5c362b8811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   index  Courses    Fee Duration  Discount\n",
      "0      2   Hadoop  26000   35days      1500\n",
      "1      0    Spark  20000   30days      1000\n",
      "2      6     Java  22000   55days      2000\n",
      "3      1  PySpark  25000   40days      2300\n",
      "4      4   pandas  24000   60days      2500\n",
      "5      5   Oracle  21000   50days      2100\n",
      "6      3   Python  22000   40days      1200\n"
     ]
    }
   ],
   "source": [
    "# Create a new Index starting from zero\n",
    "df1 = df.sample(frac = 1).reset_index()\n",
    "print(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "1b8afade-c0e4-484f-b393-835496ec4bca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Courses    Fee Duration  Discount\n",
      "0  PySpark  25000   40days      2300\n",
      "1     Java  22000   55days      2000\n",
      "2    Spark  20000   30days      1000\n",
      "3   pandas  24000   60days      2500\n",
      "4   Oracle  21000   50days      2100\n",
      "5   Hadoop  26000   35days      1500\n",
      "6   Python  22000   40days      1200\n"
     ]
    }
   ],
   "source": [
    "# Drop shuffle Index\n",
    "df1 = df.sample(frac = 1).reset_index(drop=True)\n",
    "print(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "ed18fd46-e0ce-4a29-afdc-1706fdebd6e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Discount    Fee Duration  Courses\n",
      "0      1000  20000   30days    Spark\n",
      "1      2000  22000   55days     Java\n",
      "2      2300  25000   40days  PySpark\n",
      "3      1500  26000   35days   Hadoop\n",
      "4      2100  21000   50days   Oracle\n",
      "5      1200  22000   40days   Python\n",
      "6      2500  24000   60days   pandas\n"
     ]
    }
   ],
   "source": [
    "# Using sample() method to shuffle DataFrame rows and columns\n",
    "df2 = df.sample(frac=1, axis=1).sample(frac=1).reset_index(drop=True)\n",
    "print(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af0e39a-e773-47ac-9c6e-3bdce9636a7c",
   "metadata": {},
   "source": [
    "# 11. Pandas - Join\n",
    "\n",
    "Pandas join() is similar to SQL join where it combines columns from multiple DataFrames based on row indices. **In pandas join can be done only on indexes but not on columns. If you want to join on columns you should use pandas.merge() method** as this by default performs on columns. By default, it uses the left join on the row index.\n",
    "\n",
    "\n",
    "### Syntax of pandas.DataFrame.join() method\n",
    "DataFrame.join(other, on=None, how='left', lsuffix='', rsuffix='', sort=False)\n",
    "\n",
    "* other – Pass the right DataFrame object or list of DataFrame objects.\n",
    "* on – Specify which index you want to join on when you have multiple indexes.\n",
    "* how – Use to specify the join type. Accepts inner, left, right, outer.\n",
    "* lsuffix – Specify the left suffix string to column names\n",
    "* rsuffix – Specify the right suffix string to column names\n",
    "* sort – To specify the results to be sorted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "9e131dd2-808b-4af2-bec4-6546f053d51c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First DataFrame:\n",
      "     Courses    Fee Duration\n",
      "r1    Spark  20000   30days\n",
      "r2  PySpark  25000   40days\n",
      "r3   Python  22000   35days\n",
      "r4   pandas  30000   50days\n",
      "--------------------------\n",
      "Second DataFRame:\n",
      "    Courses  Discount\n",
      "r1   Spark      2000\n",
      "r6    Java      2300\n",
      "r3  Python      1200\n",
      "r5      Go      2000\n"
     ]
    }
   ],
   "source": [
    "technologies = {\n",
    "    'Courses':[\"Spark\",\"PySpark\",\"Python\",\"pandas\"],\n",
    "    'Fee' :[20000,25000,22000,30000],\n",
    "    'Duration':['30days','40days','35days','50days'],\n",
    "              }\n",
    "index_labels=['r1','r2','r3','r4']\n",
    "df1 = pd.DataFrame(technologies,index=index_labels)\n",
    "print(\"First DataFrame:\\n\", df1)\n",
    "technologies2 = {\n",
    "    'Courses':[\"Spark\",\"Java\",\"Python\",\"Go\"],\n",
    "    'Discount':[2000,2300,1200,2000]\n",
    "              }\n",
    "index_labels2=['r1','r6','r3','r5']\n",
    "df2 = pd.DataFrame(technologies2,index=index_labels2)\n",
    "print('--------------------------')\n",
    "print(\"Second DataFRame:\\n\", df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "f0733298-bf4a-404f-a342-496cd53a6abe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After joining two DataFrames:\n",
      "    Courses_left    Fee Duration Courses_right  Discount\n",
      "r1        Spark  20000   30days         Spark    2000.0\n",
      "r2      PySpark  25000   40days           NaN       NaN\n",
      "r3       Python  22000   35days        Python    1200.0\n",
      "r4       pandas  30000   50days           NaN       NaN\n"
     ]
    }
   ],
   "source": [
    "# Pandas join \n",
    "df3=df1.join(df2, lsuffix=\"_left\", rsuffix=\"_right\")\n",
    "print(\"After joining two DataFrames:\\n\", df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "e55f8e1d-b975-42bf-b036-32b2010e04c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Courses_left    Fee Duration Courses_right  Discount\n",
      "r1        Spark  20000   30days         Spark      2000\n",
      "r3       Python  22000   35days        Python      1200\n"
     ]
    }
   ],
   "source": [
    "# Pandas Inner join DataFrames\n",
    "df3=df1.join(df2, lsuffix=\"_left\", rsuffix=\"_right\", how='inner')\n",
    "print(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "364a1ef3-597a-44f1-8c05-9e623dd33081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Courses_left      Fee Duration Courses_right  Discount\n",
      "r1        Spark  20000.0   30days         Spark      2000\n",
      "r6          NaN      NaN      NaN          Java      2300\n",
      "r3       Python  22000.0   35days        Python      1200\n",
      "r5          NaN      NaN      NaN            Go      2000\n"
     ]
    }
   ],
   "source": [
    "# Pandas Right join DataFrames\n",
    "df3=df1.join(df2, lsuffix=\"_left\", rsuffix=\"_right\", how='right')\n",
    "print(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "994f96ba-6c63-4415-801c-6c8c2d5d8466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Courses_left      Fee Duration Courses_right  Discount\n",
      "r1        Spark  20000.0   30days         Spark    2000.0\n",
      "r2      PySpark  25000.0   40days           NaN       NaN\n",
      "r3       Python  22000.0   35days        Python    1200.0\n",
      "r4       pandas  30000.0   50days           NaN       NaN\n",
      "r5          NaN      NaN      NaN            Go    2000.0\n",
      "r6          NaN      NaN      NaN          Java    2300.0\n"
     ]
    }
   ],
   "source": [
    "# Pandas outer join DataFrames\n",
    "df3=df1.join(df2, lsuffix=\"_left\", rsuffix=\"_right\", how='outer')\n",
    "print(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "d416f5ff-3295-4b46-8553-b059450c6f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Fee Duration  Discount\n",
      "Courses                          \n",
      "Spark    20000   30days      2000\n",
      "Python   22000   35days      1200\n"
     ]
    }
   ],
   "source": [
    "# Pandas join on columns\n",
    "df3=df1.set_index('Courses').join(df2.set_index('Courses'), how='inner')\n",
    "print(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "e94b7c0d-a45c-44ce-a0a7-01399756ad7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Courses    Fee Duration  Discount\n",
      "r1   Spark  20000   30days      2000\n",
      "r3  Python  22000   35days      1200\n"
     ]
    }
   ],
   "source": [
    "# Pandas join\n",
    "df3=df1.join(df2.set_index('Courses'), how='inner', on='Courses')\n",
    "print(df3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1b24a6-313d-44fd-98ed-dbec88a755bc",
   "metadata": {},
   "source": [
    "# 12. Pandas - merge DataFrames\n",
    "\n",
    "Pandas support pandas.merge() and DataFrame.merge() to merge DataFrames which is exactly similar to SQL join and supports different types of join inner, left, right, outer, cross. By default, it uses inner join where keys don’t match the rows get dropped from both DataFrames, and the result DataFrame contains rows that match on both.\n",
    "\n",
    "### Pandas.merge() Syntax\n",
    "pandas.merge(left, right, how='inner', on=None, left_on=None, right_on=None, left_index=False, right_index=False, sort=False, suffixes=('_x', '_y'), copy=True, indicator=False, validate=None)\n",
    "\n",
    "#### Pandas.DataFrame.merge() Syntax\n",
    "DataFrame.merge(right, how='inner', on=None, left_on=None, right_on=None, left_index=False, right_index=False, sort=False, suffixes=('_x', '_y'), copy=True, indicator=False, validate=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "56c88c99-df18-4060-b8aa-9dc7c48fe965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First DataFrame:\n",
      "     Courses    Fee Duration\n",
      "r1    Spark  20000   30days\n",
      "r2  PySpark  25000   40days\n",
      "r3   Python  22000   35days\n",
      "r4   pandas  30000   50days\n",
      "-----------------------------\n",
      "Second DataFrame:\n",
      "    Courses  Discount\n",
      "r1   Spark      2000\n",
      "r6    Java      2300\n",
      "r3  Python      1200\n",
      "r5      Go      2000\n"
     ]
    }
   ],
   "source": [
    "technologies = {\n",
    "    'Courses':[\"Spark\",\"PySpark\",\"Python\",\"pandas\"],\n",
    "    'Fee' :[20000,25000,22000,30000],\n",
    "    'Duration':['30days','40days','35days','50days'],\n",
    "              }\n",
    "index_labels=['r1','r2','r3','r4']\n",
    "df1 = pd.DataFrame(technologies,index=index_labels)\n",
    "print(\"First DataFrame:\\n\", df1)\n",
    "\n",
    "technologies2 = {\n",
    "    'Courses':[\"Spark\",\"Java\",\"Python\",\"Go\"],\n",
    "    'Discount':[2000,2300,1200,2000]\n",
    "              }\n",
    "index_labels2=['r1','r6','r3','r5']\n",
    "df2 = pd.DataFrame(technologies2,index=index_labels2)\n",
    "print('-----------------------------')\n",
    "print(\"Second DataFrame:\\n\", df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "72b69b13-3f43-4d93-be31-f6c0181a2fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After merging the two DataFrames:\n",
      "   Courses    Fee Duration  Discount\n",
      "0   Spark  20000   30days      2000\n",
      "1  Python  22000   35days      1200\n",
      "------------------------\n",
      "After merging the two DataFrames:\n",
      "   Courses    Fee Duration  Discount\n",
      "0   Spark  20000   30days      2000\n",
      "1  Python  22000   35days      1200\n"
     ]
    }
   ],
   "source": [
    "# Using pandas.merge()\n",
    "df3= pd.merge(df1,df2)\n",
    "print(\"After merging the two DataFrames:\\n\", df3)\n",
    "print('------------------------')\n",
    "# Using DataFrame.merge()\n",
    "df3=df1.merge(df2)\n",
    "print(\"After merging the two DataFrames:\\n\", df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "b82aaea3-7406-427c-a754-88b0e5eca804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After merging the two DataFrames:\n",
      "   Courses    Fee Duration  Discount\n",
      "0   Spark  20000   30days      2000\n",
      "1  Python  22000   35days      1200\n",
      "------------------------\n",
      "   Courses_x    Fee Duration Courses_y  Discount\n",
      "r1     Spark  20000   30days     Spark      2000\n",
      "r3    Python  22000   35days    Python      1200\n"
     ]
    }
   ],
   "source": [
    "# Merge DataFrames by Columns\n",
    "df3=pd.merge(df1,df2, on='Courses')\n",
    "\n",
    "# When column names are different\n",
    "df3=pd.merge(df1,df2, left_on='Courses', right_on='Courses')\n",
    "print(\"After merging the two DataFrames:\\n\", df3)\n",
    "print('------------------------')\n",
    "# Merge DataFrames by Index\n",
    "df3 = pd.merge(df1,df2,left_index=True,right_index=True)\n",
    "print(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "22feaa76-5666-4160-bd1a-746cb312985d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Courses_x    Fee Duration_x Courses_y  Discount Courses Duration_y\n",
      "0     Spark  20000     30days     Spark      2000   Spark     30days\n",
      "1    Python  22000     35days    Python      1200  Python     35days\n"
     ]
    }
   ],
   "source": [
    "# Use pandas.merge() to on multiple columns\n",
    "df3 = pd.merge(df3, df1,  how='left', left_on=['Courses_x','Fee'], right_on = ['Courses','Fee'])\n",
    "print(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "779df606-47e9-4583-b659-3b0dded0abaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Courses    Fee Duration  Discount\n",
      "0    Spark  20000   30days    2000.0\n",
      "1  PySpark  25000   40days       NaN\n",
      "2   Python  22000   35days    1200.0\n",
      "3   pandas  30000   50days       NaN\n"
     ]
    }
   ],
   "source": [
    "# Merge by left Join\n",
    "df3=pd.merge(df1,df2, on='Courses', how='left')\n",
    "print(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "e5d1ba12-7d97-48c6-80ce-676a594ef6c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Courses      Fee Duration  Discount\n",
      "0   Spark  20000.0   30days      2000\n",
      "1    Java      NaN      NaN      2300\n",
      "2  Python  22000.0   35days      1200\n",
      "3      Go      NaN      NaN      2000\n"
     ]
    }
   ],
   "source": [
    "# Merge by right Join\n",
    "df3=pd.merge(df1,df2, on='Courses', how='right')\n",
    "print(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "a668249e-85bc-48b6-9830-1c2711de4f46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Courses      Fee Duration  Discount\n",
      "0    Spark  20000.0   30days    2000.0\n",
      "1  PySpark  25000.0   40days       NaN\n",
      "2   Python  22000.0   35days    1200.0\n",
      "3   pandas  30000.0   50days       NaN\n",
      "4     Java      NaN      NaN    2300.0\n",
      "5       Go      NaN      NaN    2000.0\n"
     ]
    }
   ],
   "source": [
    "# Merge by outer Join\n",
    "df3=pd.merge(df1,df2, on='Courses', how='outer')\n",
    "print(df3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb4d7df-6edc-4024-b4af-e3a926d6f42c",
   "metadata": {},
   "source": [
    "# 13. Pandas - Concat DataFrames\n",
    "\n",
    "You can use the pandas.concat() function to concatenate or merge two or more pandas DataFrames either along rows or columns.  When concatenating DataFrames along rows, concat() creates a new DataFrame that includes all rows from the input DataFrames, effectively appending one DataFrame to another. Conversely, when concatenating along columns, concat() performs a join operation, combining the DataFrames side-by-side based on their indexes.\n",
    "\n",
    "### Syntax of concat() function\n",
    "pandas.concat(objs, axis=0, join='outer', ignore_index=False, keys=None, levels=None, names=None, verify_integrity=False, sort=False, copy=True)\n",
    "\n",
    "\n",
    "* objs – This is a sequence or mapping of Series or DataFrame objects. If a dictionary is passed, the keys will be used to construct a hierarchical index.   \n",
    "* axis – {0 or ‘index’, 1 or ‘columns’}, default 0. The axis concatenates along. 0 or 'index' means concatenate along rows (i.e., vertically). 1 or 'columns' means concatenate along columns (i.e., horizontally).    \n",
    "* join – Type of join to be performed. It can be ‘inner’ or ‘outer’. Defaults to ‘outer’.    \n",
    "* ignore_index – If True, do not use the index values along the concatenation axis. Defaults to False.    \n",
    "* keys – Values to associate with the concatenated objects along the concatenation axis. It’s useful for creating a hierarchical index.    \n",
    "* levels – Specific level(s) (zero-indexed) from the keys to use as index levels.\n",
    "* names – Names for the levels in the resulting hierarchical index.    \n",
    "* verify_integrity – If True, check whether the new concatenated axis contains duplicates. Defaults to False.    \n",
    "* sort – If True, sort the resulting DataFrame by the labels along the concatenation axis. Defaults to False.    \n",
    "* copy – If False, avoid copying data unnecessarily. Defaults to True.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "f5dec78d-79f6-4ef9-8e37-bdab3642b6ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First DataFrame:\n",
      "    Courses    Fee\n",
      "0    Spark  20000\n",
      "1  PySpark  25000\n",
      "2   Python  22000\n",
      "3   pandas  24000\n",
      "----------------\n",
      "Second DataFrame:\n",
      "     Courses    Fee\n",
      "0    Pandas  25000\n",
      "1    Hadoop  25200\n",
      "2  Hyperion  24500\n",
      "3      Java  24900\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame({'Courses': [\"Spark\",\"PySpark\",\"Python\",\"pandas\"],\n",
    "                    'Fee' : [20000,25000,22000,24000]})\n",
    "\n",
    "df1 = pd.DataFrame({'Courses': [\"Pandas\",\"Hadoop\",\"Hyperion\",\"Java\"],\n",
    "                    'Fee': [25000,25200,24500,24900]})\n",
    "print(\"First DataFrame:\\n\", df)\n",
    "print('----------------')\n",
    "print(\"Second DataFrame:\\n\", df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "b5ebde38-b6c6-4534-8db3-c321c947bf82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After concatenating the two DataFrames:\n",
      "     Courses    Fee\n",
      "0     Spark  20000\n",
      "1   PySpark  25000\n",
      "2    Python  22000\n",
      "3    pandas  24000\n",
      "0    Pandas  25000\n",
      "1    Hadoop  25200\n",
      "2  Hyperion  24500\n",
      "3      Java  24900\n"
     ]
    }
   ],
   "source": [
    "# Using pandas.concat() to concat two DataFrames\n",
    "data = [df, df1]\n",
    "df2 = pd.concat(data)\n",
    "print(\"After concatenating the two DataFrames:\\n\", df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "53bfd725-b7d9-4e9d-b8c0-97f26103d44c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Courses    Fee\n",
      "0     Spark  20000\n",
      "1   PySpark  25000\n",
      "2    Python  22000\n",
      "3    pandas  24000\n",
      "4    Pandas  25000\n",
      "5    Hadoop  25200\n",
      "6  Hyperion  24500\n",
      "7      Java  24900\n"
     ]
    }
   ],
   "source": [
    "# Use pandas.concat() method to ignore_index \n",
    "df2 = pd.concat([df, df1], ignore_index=True, sort=False)\n",
    "print(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a298ce-fc42-40a0-a416-4b6ac0f7d192",
   "metadata": {},
   "source": [
    "# 14. Pandas - .fillna()\n",
    "\n",
    "pandas.DataFrame.fillna() method is used to fill column (one or multiple columns) containing NA/NaN/None with 0, empty, blank, or any specified values etc. NaN is considered a missing value. When you dealing with machine learning, handling missing values is very important, not handling these will result in a side effect with an incorrect result.\n",
    "\n",
    "### Syntax of pandas.DataFrame.fillna()\n",
    "DataFrame.fillna(value=None, method=None, axis=None, inplace=False, limit=None, downcast=None)\n",
    "\n",
    "* value – Takes either scalar, dict, Series, or DataFrame but not list.\n",
    "* method – Takes one of these values {‘backfill’, ‘bfill’, ‘pad’, ‘ffill’, None}. Default None.\n",
    "* axis – 0 or ‘index’, 1 or ‘columns’. Used to specify the axis to fill the values.\n",
    "* inplace – Default False. When used True, it updates the existing DataFrame object.\n",
    "* limit – Specify how many fills should happen. This is the maximum number of consecutive NaN values replaced with specified values.\n",
    "* downcast – It takes a dict of key-value pair that specifies data type to downcast. Like Float64 to int64, date to string e.t.c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "c6617256-e485-440e-9033-7ea5d3e4f2c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create DataFrame:\n",
      "   Courses      Fee Duration  Discount\n",
      "0   Spark  20000.0   30days    1000.0\n",
      "1    Java      NaN   40days       NaN\n",
      "2   Scala  26000.0     <NA>    2500.0\n",
      "3  Python  24000.0   40days       NaN\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(({\n",
    "     'Courses':[\"Spark\",'Java',\"Scala\",'Python'],\n",
    "     'Fee' :[20000,np.nan,26000,24000],\n",
    "     'Duration':['30days','40days', pd.NA,'40days'],\n",
    "     'Discount':[1000,np.nan,2500,None]\n",
    "               }))\n",
    "print(\"Create DataFrame:\\n\", df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "31b163a0-2cad-4123-bbc5-f539889988ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After replacing all NAN/NA values with None:\n",
      "   Courses      Fee Duration Discount\n",
      "0   Spark  20000.0   30days   1000.0\n",
      "1    Java     None   40days     None\n",
      "2   Scala  26000.0     None   2500.0\n",
      "3  Python  24000.0   40days     None\n"
     ]
    }
   ],
   "source": [
    "# Fillna to replace all NaN\n",
    "df2 = df.fillna('None')\n",
    "print(\"After replacing all NAN/NA values with None:\\n\", df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "22fc2b3d-3d02-430d-81fb-b162f5109eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Courses      Fee Duration Discount\n",
      "0   Spark  20000.0   30days   1000.0\n",
      "1    Java     None   40days        0\n",
      "2   Scala  26000.0     None   2500.0\n",
      "3  Python  24000.0   40days        0\n"
     ]
    }
   ],
   "source": [
    "# Fillna on one column\n",
    "df2['Discount'] =  df['Discount'].fillna('0')\n",
    "print(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "9c4e7f20-0378-4037-b6d4-0fabedcfe480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Courses      Fee Duration Discount\n",
      "0   Spark  20000.0   30days   1000.0\n",
      "1    Java        0   40days        0\n",
      "2   Scala  26000.0     None   2500.0\n",
      "3  Python  24000.0   40days        0\n"
     ]
    }
   ],
   "source": [
    "# Fillna() on multiple columns\n",
    "df2[['Discount','Fee']] =  df[['Discount','Fee']].fillna('0')\n",
    "print(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "3eaadfb8-c22a-4492-be7d-8537abe32c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Courses      Fee Duration Discount\n",
      "0   Spark  20000.0   30days   1000.0\n",
      "1    Java  10000.0   40days        0\n",
      "2   Scala  26000.0     <NA>   2500.0\n",
      "3  Python  24000.0   40days        0\n"
     ]
    }
   ],
   "source": [
    "# Fillna() on multiple columns\n",
    "df2 =  df.fillna(value={'Discount':'0','Fee':10000})\n",
    "print(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "2eb1b735-7f34-4fd3-b541-75c51f0c59d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Courses      Fee Duration  Discount\n",
      "0   Spark  20000.0   30days    1000.0\n",
      "1    Java      0.0   40days       0.0\n",
      "2   Scala  26000.0     <NA>    2500.0\n",
      "3  Python  24000.0   40days       NaN\n"
     ]
    }
   ],
   "source": [
    "# Fill with limit\n",
    "df2=df.fillna(value={'Discount':0,'Fee':0},limit=1)\n",
    "print(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a04ef03-6f97-430d-bbe7-a29e642676a7",
   "metadata": {},
   "source": [
    "# 15. Pandas - .dropna() \n",
    "\n",
    "pandas.DataFrame.dropna() is used to drop/remove missing values from rows and columns, np.nan/pd.NaT (Null/None) are considered as missing values. Before we process the data, it is very important to clean up the missing data, as part of cleaning we would be required to identify the rows with Null/NaN/None values and drop them. This dropna() method comes in handy to drop rows with np.nan/pd.NaT values.\n",
    "\n",
    "### Pandas.DataFrame.dropna() syntax\n",
    "DataFrame.dropna(axis=0, how='any', thresh=None, subset=None, inplace=False)\n",
    "\n",
    "* pandas.DataFrame.dropna() is used to drop columns with NaN/None values from DataFrame.  \n",
    "* numpy.nan is Not a Number (NaN), which is of Python build-in numeric type float (floating point).  \n",
    "* Set axis=1 to drop columns containing NaN values instead of rows.  \n",
    "* None is of NoneType and it is an object in Python.  \n",
    "* Use how='all' to remove rows or columns only if every entry is NaN.  \n",
    "Specify thresh to keep rows or columns that meet a minimum count of non-NaN values.\n",
    "* Apply dropna() conditionally by specifying columns in subset where non-NaN values are required.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "fc1b7c77-cb46-4233-b402-26eaf5dd3e90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Courses      Fee Duration Discount    \n",
      "r1    Spark  20000.0    30day     1000 NaN\n",
      "r2  PySpark  25000.0   40days      NaN NaN\n",
      "r3   Hadoop  26000.0   35days     1200 NaN\n",
      "r4   Python  23093.0   45days     2500 NaN\n",
      "r5   pandas  24000.0      NaN      NaT NaN\n",
      "        NaN      NaN      NaN      NaN NaN\n"
     ]
    }
   ],
   "source": [
    "technologies = {\n",
    "    'Courses':[\"Spark\",\"PySpark\",\"Hadoop\",\"Python\",\"pandas\",np.nan],\n",
    "    'Fee' :[20000,25000,26000,23093,24000,np.nan],\n",
    "    'Duration':['30day','40days','35days','45days',np.nan,np.nan],\n",
    "    'Discount':[1000,np.nan,1200,2500,pd.NaT,np.nan],\n",
    "    '':[np.nan,np.nan,np.nan,np.nan,np.nan,np.nan]\n",
    "              }\n",
    "index_labels=['r1','r2','r3','r4','r5','']\n",
    "df = pd.DataFrame(technologies,index=index_labels)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "eeb3649c-951a-4a42-ab5e-77a25343948f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Courses      Fee Duration Discount    \n",
      "r1    Spark  20000.0    30day     1000 NaN\n",
      "r2  PySpark  25000.0   40days      NaN NaN\n",
      "r3   Hadoop  26000.0   35days     1200 NaN\n",
      "r4   Python  23093.0   45days     2500 NaN\n",
      "r5   pandas  24000.0      NaN      NaT NaN\n"
     ]
    }
   ],
   "source": [
    "# Drop rows that has all Nan Values\n",
    "df = df.dropna(how='all')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "12259eee-e68d-4383-80d4-f89401db6fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Courses      Fee Duration Discount\n",
      "r1    Spark  20000.0    30day     1000\n",
      "r2  PySpark  25000.0   40days      NaN\n",
      "r3   Hadoop  26000.0   35days     1200\n",
      "r4   Python  23093.0   45days     2500\n",
      "r5   pandas  24000.0      NaN      NaT\n"
     ]
    }
   ],
   "source": [
    "# Drop columns that has all Nan Values\n",
    "df = df.dropna(how='all',axis=1)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "5e342540-ad04-4574-bb50-71d56b5d7073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Courses      Fee Duration Discount\n",
      "r1   Spark  20000.0    30day     1000\n",
      "r3  Hadoop  26000.0   35days     1200\n",
      "r4  Python  23093.0   45days     2500\n"
     ]
    }
   ],
   "source": [
    "# Drop rows that contains nan values\n",
    "df2=df.dropna()\n",
    "print(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "909995ba-2cea-4780-af6c-8dacb125f7e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Courses      Fee\n",
      "r1    Spark  20000.0\n",
      "r2  PySpark  25000.0\n",
      "r3   Hadoop  26000.0\n",
      "r4   Python  23093.0\n",
      "r5   pandas  24000.0\n"
     ]
    }
   ],
   "source": [
    "# Drop columns that contains nan values\n",
    "df2=df.dropna(axis=1)\n",
    "print(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "38c98f7b-c74e-4d67-b88c-1c77260af61b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Courses      Fee Duration Discount\n",
      "r1    Spark  20000.0    30day     1000\n",
      "r2  PySpark  25000.0   40days      NaN\n",
      "r3   Hadoop  26000.0   35days     1200\n",
      "r4   Python  23093.0   45days     2500\n"
     ]
    }
   ],
   "source": [
    "# Drop rows that has NaN values on selected columns\n",
    "df2=df.dropna(subset=['Courses','Duration'])\n",
    "print(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "bf715d27-fbec-4ba8-9371-3af2459a0a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With threshold, \n",
    "# Keep only the rows with at least 2 non-NA values.\n",
    "df2=df.dropna(thresh=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
